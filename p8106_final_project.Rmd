---
title: 'P8106 Final Project: Predicting COVID-19 Recovery Time and Identifying Significant Risk Factors'
author: "Runze Cui (rc3521), Yuchen Hua (yh3555), Hongpu Min (hm2946)"
date: "2023-05-01"
output: 
  pdf_document:
    latex_engine: xelatex
    toc: yes
    toc_depth: 2
editor_options: 
  chunk_output_type: console
  header-includes:
   -\usepackage{fancyhdr}
   -\usepackage{lipsum}
   -\pagestyle{fancy}
   -\fancyhead[R]{\thepage}
   -\fancypagestyle{plain}{\pagestyle{fancy}}
--- 

\newpage


```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, collapse = TRUE, message = FALSE, warning = FALSE, dpi = 300, fig.width = 7)
```

```{r package, include=F}
library(tidyverse)
library(ggplot2)
library(caret)
library(mgcv)
library(earth)
library(AppliedPredictiveModeling)
library(vip)
library(patchwork)
library(rpart.plot)
library(rpart)
library(ranger)
library(gbm)
library(parallel)
library(doParallel)
library(pROC)
```


# Background

[Check the report]

# Data: 

[Description check the report]

```{r}
# For primary analysis: 
# Dataset Loading:
load("data/recovery.Rdata")

set.seed(3521) # Runze Cui's uni(2183): 
# Create a first random sample of 2000 participants: 
dat1 <- dat[sample(1:10000, 2000),] 

set.seed(3555) # Yuchen Hua's uni(3555)
# Create a second random sample of 2000 participants: 
dat2 <- dat[sample(1:10000, 2000),] 

# Merged the two datasets and remove repeated observations: 
dat <- unique(rbind(dat1, dat2))

# Get rid of the id variable from the merged dataset and do the data cleaning: 
dat <- dat %>% 
  select(-id) %>% 
  mutate(gender = as.factor(gender)) %>% 
  mutate(race = as.factor(race)) %>% 
  mutate(smoking = as.factor(smoking)) %>% 
  mutate(hypertension = as.factor(hypertension)) %>% 
  mutate(diabetes = as.factor(diabetes)) %>% 
  mutate(vaccine = as.factor(vaccine)) %>% 
  mutate(severity = as.factor(severity)) %>% 
  mutate(study = as.factor(study)) %>% 
  na.omit() %>% 
  relocate(recovery_time)

head(dat)

# Separate the data as training and test data: 
set.seed(3521)
# Specify rows of training data: 
trRows <- createDataPartition(dat$recovery_time, p = 0.7, list = FALSE)


# Training data:
training <- dat[trRows, ]
## Covariates' matrix:
x <- model.matrix(recovery_time ~ ., dat)[trRows, -1]
## Response's vector:
y <- dat$recovery_time[trRows]

# Test data:
test <- dat[-trRows, ]
## Covariates' matrix:
x2 <- model.matrix(recovery_time ~ ., dat)[-trRows, -1]
## Response's vector:
y2 <- dat$recovery_time[-trRows]




# For secondary analysis: 
dat_2 <- dat %>%
  mutate(recovery_time = ifelse(recovery_time > 30, "great", "less")) %>%
  mutate(recovery_time = as.factor(recovery_time))

# Training data:
training_sec <- dat_2[trRows, ]
## Covariates' matrix:
x_sec <- model.matrix(recovery_time ~ ., dat_2)[trRows, -1]
## Response's vector:
y_sec <- dat_2$recovery_time[trRows]

# Test data:
test_sec <- dat_2[-trRows, ]
## Covariates' matrix:
x2_sec <- model.matrix(recovery_time ~ ., dat_2)[-trRows, -1]
## Response's vector:
y2_sec <- dat_2$recovery_time[-trRows]
```



# Exploratory Analysis and Data Visualization

[Description check the report] 

## Exploratory Analysis

```{r Exploratory Analysis, warning=F, message=F}
# Summary statistics for whole dataset:
summary(dat)
# Summary tables separated by continuous/categorical variables:
skimr::skim(dat)
```

## Data Visualization

```{r}
# For primary analysis: 
## For continuous variables: 
theme = trellis.par.get()
theme$plot.symbol$col = rgb(.2, .4, .2, .5)
theme$plot.symbol$pch = 16
theme$plot.line$col = rgb(.8, .1, .1, 1)
theme$plot.line$lwd = 2
theme$strip.background$col = rgb(.0, .2, .6, .2)
trellis.par.set(theme)

featurePlot(x = dat %>% dplyr::select(age, height, weight, bmi, SBP, LDL), 
            y = dat$recovery_time,
            plot = "scatter",
            span = .5,
            labels = c("Predictors","Y"),
            main = "Figure 1.1. Lattice Plots for Continuous Variables in Primary Analysis",
            type = c("p", "smooth"))

## For categorical variables: 
gender_plot = dat %>%
  ggplot(aes(x = gender, y = recovery_time, fill = gender)) + 
  geom_violin(color = "black", alpha = .5) + 
  scale_x_discrete(labels = c('Female','Male')) +
  ylab("Recovery") +
  theme(legend.position = "none")

race_plot = dat %>%
  ggplot(aes(x = race, y = recovery_time, fill = race)) + 
  geom_violin(color = "black", alpha = .5) + 
  scale_x_discrete(labels = c('White','Asian','Black', 'Hispanic')) +
  ylab("Recovery") +
  theme(legend.position = "none")

smoking_plot = dat %>%
  ggplot(aes(x = smoking, y = recovery_time, fill = smoking)) + 
  geom_violin(color = "black", alpha = .5) +
  scale_x_discrete(labels = c('Never smoked','Former smoker','Current smoker')) +
  ylab("Recovery") +
  theme(legend.position = "none")

hyper_plot = dat %>%
  ggplot(aes(x = hypertension, y = recovery_time, fill = hypertension)) + 
  geom_violin(color = "black", alpha = .5) +
  ylab("Recovery") +
  scale_x_discrete(labels = c('No','Yes')) +
  theme(legend.position = "none")

diabetes_plot = dat %>%
  ggplot(aes(x = diabetes, y = recovery_time, fill = diabetes)) + 
  geom_violin(color = "black", alpha = .5) +
  scale_x_discrete(labels = c('No','Yes')) +
  ylab("Recovery") +
  theme(legend.position = "none")

vac_plot = dat %>%
  ggplot(aes(x = vaccine, y = recovery_time, fill = vaccine)) + 
  geom_violin(color = "black", alpha = .5) +
  scale_x_discrete(labels = c('Not vaccinated','Vaccinated')) +
  ylab("Recovery") +
  theme(legend.position = "none")

severity_plot = dat %>%
  ggplot(aes(x = severity, y = recovery_time, fill = severity)) + 
  geom_violin(color = "black", alpha = .5) +
  scale_x_discrete(labels = c('Not severe','Severe')) + 
  ylab("Recovery") +
  theme(legend.position = "none")

study_plot = dat %>%
  ggplot(aes(x = study, y = recovery_time, fill = study)) + 
  geom_violin(color = "black", alpha = .5) +
  ylab("Recovery") +
  theme(legend.position = "none")

(gender_plot + race_plot + smoking_plot + hyper_plot) / (diabetes_plot + vac_plot + severity_plot + study_plot) + 
  plot_layout(guides = "collect") + 
  plot_annotation(title = "Figure 1.2. Violin Plots for Categorical Variables in Primary Analysis")

## Correlation matrix for continuous variables ONLY: 
model.matrix( ~ 0+., data = dat %>% dplyr::select(age, height, weight, bmi, SBP, LDL)) %>% 
  cor(use = "pairwise.complete.obs") %>% 
  ggcorrplot::ggcorrplot(show.diag = T, 
                         type = "lower", 
                         lab = F,
                         colors = c("#6D9EC1", "white", "#E46726")) + 
  ggtitle("Figure 1.3. Correlation matrix for continuous datasat")


# For secondary analysis: 
## For continuous variables: 
theme = trellis.par.get()
theme$plot.symbol$col = rgb(.2, .4, .2, .5)
theme$plot.symbol$pch = 16
theme$plot.line$col = rgb(.8, .1, .1, 1)
theme$plot.line$lwd = 2
theme$strip.background$col = rgb(.0, .2, .6, .2)
trellis.par.set(theme)

featurePlot(x = dat_2 %>% dplyr::select(age, height, weight, bmi, SBP, LDL), 
            y = dat_2$recovery_time,
            plot = "box", pch = "|", 
            scales = list(x = list(relation = "free"),
                          y = list(relation = "free")),
            labels = c("Predictors","Y"),
            main = "Figure 1.4. Lattice Plots for Continuous Variables in Secondary Analysis", 
            auto.key = list(columns = 2))

## For categorical variables: 
gender_plot_sec = dat_2 %>%
  ggplot(aes(x = gender, fill = recovery_time)) + 
  geom_bar(color = "black", alpha = .5) + 
  scale_x_discrete(labels = c('Female','Male')) +
  ylab("Recovery") +
  theme(legend.position = "none")

race_plot_sec = dat_2 %>%
  ggplot(aes(x = race, fill = recovery_time)) + 
  geom_bar(color = "black", alpha = .5) + 
  scale_x_discrete(labels = c('White','Asian','Black', 'Hispanic')) +
  ylab("Recovery") +
  theme(legend.position = "none")

smoking_plot_sec = dat_2 %>%
  ggplot(aes(x = smoking, fill = recovery_time)) + 
  geom_bar(color = "black", alpha = .5) +
  scale_x_discrete(labels = c('Never smoked','Former smoker','Current smoker')) +
  ylab("Recovery") +
  theme(legend.position = "none")

hyper_plot_sec = dat_2 %>%
  ggplot(aes(x = hypertension, fill = recovery_time)) + 
  geom_bar(color = "black", alpha = .5) +
  ylab("Recovery") +
  scale_x_discrete(labels = c('No','Yes')) +
  theme(legend.position = "none")

diabetes_plot_sec = dat_2 %>%
  ggplot(aes(x = diabetes, fill = recovery_time)) + 
  geom_bar(color = "black", alpha = .5) +
  scale_x_discrete(labels = c('No','Yes')) +
  ylab("Recovery") +
  theme(legend.position = "none")

vac_plot_sec = dat_2 %>%
  ggplot(aes(x = vaccine, fill = recovery_time)) + 
  geom_bar(color = "black", alpha = .5) +
  scale_x_discrete(labels = c('Not vaccinated','Vaccinated')) +
  ylab("Recovery") +
  theme(legend.position = "none")

severity_plot_sec = dat_2 %>%
  ggplot(aes(x = severity, fill = recovery_time)) + 
  geom_bar(color = "black", alpha = .5) +
  scale_x_discrete(labels = c('Not severe','Severe')) + 
  ylab("Recovery") +
  theme(legend.position = "none")

study_plot_sec = dat_2 %>%
  ggplot(aes(x = study, fill = recovery_time)) + 
  geom_bar(color = "black", alpha = .5) +
  ylab("Recovery") +
  theme(legend.position = "none")

(gender_plot_sec + race_plot_sec + smoking_plot_sec + hyper_plot_sec) / (diabetes_plot_sec + vac_plot_sec + severity_plot_sec + study_plot_sec) + 
  plot_layout(guides = "collect") + 
  plot_annotation(title = "Figure 1.5. Bar Plots for Categorical Variables in Secondary Analysis")
```

Note: Red is recovery time less than and equal to 30. Blue is greater than 30. 

ctrl and parallel computing setup:

```{r}
ctrl = trainControl(method = "repeatedcv", number = 10, repeats = 5)
ctrl1 = trainControl(method = "repeatedcv",
                     summaryFunction = twoClassSummary,
                     classProbs = TRUE)
no_cores = detectCores() - 1
```

# Primary Analysis

Recovery time as continuous variable. 

## Linear methods: 

### Linear model:

```{r}
set.seed(3521)
# Fit a linear regression model:
lm = train(recovery_time ~ age + gender + race + smoking + height + 
                        weight + bmi + hypertension + diabetes + SBP + 
                        LDL + vaccine + severity + study, 
               data = training, ## use training dataset
               method = "lm", 
               trControl = ctrl)
summary(lm)
# Importances: 
plot(varImp(lm, scale = TRUE), main = "Figure 2. Linear Model Variable's Importance Plot")

# Report the CV error:
lm$results$RMSE
# Report the test error:
lm_pred = predict(lm, newdata = test) ## Test dataset
lm_mse = sqrt(mean((lm_pred - y2)^2))
lm_mse
```

[Describe the outputs above]

### LASSO model:

```{r}
set.seed(3521)
# Fit a LASSO model:
lasso = train(x = x, 
              y = y, ## training dataset
              method = "glmnet",
              tuneGrid = expand.grid(alpha = 1,
                                     lambda = exp(seq(-8, 1, length = 100))),
              trControl = ctrl)
# Plot for tuning parameter selection:
plot(lasso, xTrans = log, main = "Figure 3. LASSO Model CV RMSE Plot", highlight = T)
# Choose best tuning parameter value:
lasso$bestTune
# Report the coefficients after applying the best tuning parameter:
coef(lasso$finalModel, lasso$bestTune$lambda)

# Report the CV error:
lasso$results[37, ]$RMSE
# Report the test error:
lasso_pred = predict(lasso, newdata = x2) ## Test dataset
lasso_mse = sqrt(mean((lasso_pred - y2)^2))
lasso_mse
```

[Describe the outputs above]

### Ridge model:

```{r}
set.seed(3521)
# Fit a ridge model:
ridge = train(x = x, 
              y = y, ## training dataset
              method = "glmnet",
              tuneGrid = expand.grid(alpha = 0,
                                     lambda = exp(seq(-4, 1, length = 100))),
              trControl = ctrl)
# Plot for tuning parameter selection:
plot(ridge, xTrans = log, main = "Figure 4. Ridge Model CV RMSE Plot", highlight = T)
# Choose best tuning parameter value:
ridge$bestTune
# Report the coefficients after applying the best tuning parameter:
coef(ridge$finalModel, ridge$bestTune$lambda)

# Report the CV error:
ridge$results[73, ]$RMSE
# Report the test error:
ridge_pred = predict(ridge, newdata = x2) ## Test dataset
ridge_mse = sqrt(mean((ridge_pred - y2)^2))
ridge_mse
```

[Describe the outputs above]

### Elastic net model:

```{r}
set.seed(3521)
# Fit a Elastic net model:
enet = train(x, y, ## training dataset
             method = "glmnet",
             tuneGrid = expand.grid(alpha = seq(0, 1, length = 21),
                                    lambda = exp(seq(-8, -2, length = 50))),
             trControl = ctrl)
# Plot for tuning parameters selection:
myCol = rainbow(25)
myPar = list(superpose.symbol = list(col = myCol),
             superpose.line = list(col = myCol))
plot(enet, par.settings = myPar, main = "Figure 5. Elastic Net Model CV RMSE Plot", xTrans = log)
# Choose best tuning parameter value:
enet$bestTune
# Report the coefficients after applying the best tuning parameter:
coef(enet$finalModel, enet$bestTune$lambda)

# Report the CV error:
enet$results[220, ]$RMSE
# Report the test error:
enet_pred = predict(enet, newdata = x2) ## Test dataset
enet_mse = sqrt(mean((enet_pred - y2)^2))
enet_mse
```

[Describe the outputs above]

### Partial least squares model (PLS): 

```{r}
set.seed(3521)
# Fit a PLS model:
pls = train(x, y,  ## training dataset
            method = "pls",
            tuneGrid = data.frame(ncomp = 1:18),
            trControl = ctrl,
            preProcess = c("center", "scale"))
summary(pls)
# Choose best tuning parameter value:
pls$bestTune
# Plot for the number of components:
ggplot(pls, highlight = T) + 
  theme_bw() +
  ggtitle("Figure 6. PLS Model CV RMSE Plot")

# Report the CV error:
pls$results[11, ]$RMSE
# Report the test error:
pls_pred = predict(pls, newdata = x2) ## Test dataset
pls_mse = sqrt(mean((pls_pred - y2)^2))
pls_mse
```

[Describe the outputs above]

## Nonlinear Methods: 

### Generalized additive model (GAM): 

```{r}
set.seed(3521)
# Fit a GAM model for all predictors:
gam = train(x, y, ## training dataset
            method = "gam",
            trControl = ctrl)
summary(gam)
# Report the model:
gam$bestTune
gam$finalModel
# Plot for tuning parameter selection:
plot(gam, main = "Figure 7. GAM Model CV RMSE Plot")

# Report the CV error:
gam$results$RMSE
# Report the test error:
gam_pred = predict(gam, newdata = x2) ## Test dataset
gam_mse = sqrt(mean((gam_pred - y2)^2))
gam_mse
```

[Describe the outputs above]

### Multivariate adaptive regression spline (MARS) model:

```{r}
set.seed(3521)
# Create grid for two tuning parameters in MARS
mars_grid = expand.grid(degree = 1:3,  ## number of possible product hinge functions
                         nprune = 1:20) ##  the number of basis functions to be retained after the pruning process
# Fit the MARS model:
cl = makePSOCKcluster(no_cores)
registerDoParallel(cl)
mars = train(x, y, # training dataset
              method = "earth",
              tuneGrid = mars_grid,
              trControl = ctrl)
stopCluster(cl)
registerDoSEQ()

# Report the coefficients:
coef(mars$finalModel)
# Plot for tuning parameter selection:
ggplot(mars, highlight = T) + 
  theme_bw() +
  ggtitle("Figure 8. MARS Model CV RMSE Plot")
# Choose best tuning parameter value:
mars$bestTune

# Report the CV error:
mars$results[27, ]$RMSE
# Report test error:
mars_pred = predict(mars, newdata = x2) ## Test dataset
mars_mse = sqrt(mean((mars_pred - y2)^2))
mars_mse
```

[Describe the outputs above]

### Regression Tree: 

```{r}
set.seed(3521)
# Fit the regression tree model:
cl = makePSOCKcluster(no_cores)
registerDoParallel(cl)
reg_tree = train(x, y, ## training dataset
                 method = "rpart",
                 tuneGrid = data.frame(cp = exp(seq(-6, -2, length = 50))),
                 trControl = ctrl)
stopCluster(cl)
registerDoSEQ()
# Plot the regression tree:
rpart.plot(reg_tree$finalModel, main = "Figure 9.1. Regression Tree")
# Plot for tuning parameter selection:
ggplot(reg_tree, highlight = TRUE) + ggtitle("Figure 9.2 Regression Tree Model CV RMSE Plot")
# Choose best tuning parameter value:
reg_tree$bestTune

# Report the CV error:
reg_tree$results[12, ]$RMSE
# Report test error:
reg_tree_pred = predict(reg_tree, newdata = x2) ## Test dataset
reg_tree_mse = sqrt(mean((reg_tree_pred - y2)^2))
reg_tree_mse
```

[Describe the outputs above]

### Random Forests: 

```{r}
set.seed(3521)
# Fit the random forest model:
rf.grid = expand.grid(mtry = 1:14, ## We have 14 predictors in training data in total
                      splitrule = "variance",
                      min.node.size = 1:6) ## large/small model flexibility
set.seed(3521)
cl = makePSOCKcluster(no_cores)
registerDoParallel(cl)
rf = train(x, y,
           method = "ranger",
           tuneGrid = rf.grid,
           trControl = ctrl)
stopCluster(cl)
registerDoSEQ()
# Plot for tuning parameter selection:
ggplot(rf, highlight = TRUE) + ggtitle("Figure 10. Random Forests CV RMSE Plot")
# Choose best tuning parameter value:
rf$bestTune

# Report the CV error:
rf$results[42, ]$RMSE
# Report test error:
rf_pred = predict(rf, newdata = x2) ## Test dataset
rf_mse = sqrt(mean((rf_pred - y2)^2))
rf_mse
```

[Describe the outputs above]

### Boosting: 

```{r}
set.seed(3521)
boosting.grid = expand.grid(n.trees = c(2000, 3000, 4000, 5000),
                       interaction.depth = 1:3,
                       shrinkage = c(0.001, 0.003, 0.005),
                       n.minobsinnode = 1)
set.seed(3521)
cl = makePSOCKcluster(no_cores)
registerDoParallel(cl)
boosting = train(x, y,
                method = "gbm",
                tuneGrid = boosting.grid,
                trControl = ctrl,
                verbose = FALSE)
stopCluster(cl)
registerDoSEQ()

# Plot for tuning parameter selection:
ggplot(boosting, highlight = TRUE) + ggtitle("Figure 11. Boosting CV RMSE Plot")
# Choose best tuning parameter value:
boosting$bestTune

# Report the CV error:
boosting$results[26, ]$RMSE
# Report test error:
boosting_pred = predict(boosting, newdata = x2) ## Test dataset
boosting_mse = sqrt(mean((boosting_pred - y2)^2))
boosting_mse
```

[Describe the outputs above]







# Secondary Analysis

### Logistic Model: 

```{r}
set.seed(3521)
glm = train(recovery_time ~ ., 
            data = training_sec,
            method = "glm", 
            metric = "ROC", 
            trControl = ctrl1)
glm$finalModel

# Report the CV ROC:
glm$results$ROC
```

[Describe the outputs above]

### Penalized Logistic Model: 

```{r}
set.seed(3521)
glmnGrid = expand.grid(alpha = seq(0, 1, length = 18), 
                       lambda = exp(seq(-8, -1, length = 50)))

glmn = train(recovery_time ~ ., 
             data = training_sec,
             method = "glmnet", 
             tuneGrid = glmnGrid, 
             metric = "ROC",
             trControl = ctrl1)
# Plot for tuning parameter selection:
myCol = rainbow(25)
myPar = list(superpose.symbol = list(col = myCol),
        superpose.line = list(col = myCol))
plot(glmn, par.settings = myPar, xTran = function(x)log(x), highlight = T, main = "Figure I. Penalized Logistic Model CV ROC Plot")
# Choose best tuning parameter value:
glmn$bestTune

# Report the CV ROC:
glmn$results[851, ]$ROC
```

### GAM model for binary response:

```{r}
set.seed(3521)
cl = makePSOCKcluster(no_cores)
registerDoParallel(cl)
gam_2 = train(recovery_time ~ ., 
              data = training_sec,
              method = "gam", 
              metric = "ROC", 
              trControl = ctrl1)
stopCluster(cl)
registerDoSEQ()
# Plot for tuning parameter selection:
plot(gam_2, main = "Figure II. GAM Model CV ROC Plot")
# Report the final model: 
gam_2$bestTune
coef(gam_2$finalModel)

# Report the CV ROC:
gam_2$results[2, ]$ROC
```

### MARS model for binary response: 

```{r}
set.seed(3521)
cl = makePSOCKcluster(no_cores)
registerDoParallel(cl)
mars_2 = train(recovery_time ~ ., 
               data = training_sec,
               method = "earth", 
               tuneGrid = expand.grid(degree = 1:4, nprune = 2:14),
               metric = "ROC", 
               trControl = ctrl1)
stopCluster(cl)
registerDoSEQ()
# Plot for tuning parameter selection:
plot(mars_2, main = "Figure III. MARS Model CV ROC Plot", highlight = T)
# Report the final model:
mars_2$bestTune
coef(mars_2$finalModel)

# Report the CV ROC:
mars_2$results[37, ]$ROC
```

### Linear discriminant analysis (LDA):

```{r}
set.seed(3521)
lda = train(recovery_time ~ ., 
            data = training_sec,
            method = "lda",
            metric = "ROC",
            trControl = ctrl1)
# Report the final model:
coef(lda$finalModel)

# Report the CV ROC:
lda$results$ROC
```

### Quadratic discriminant analysis (QDA):

```{r}
set.seed(3521)
qda = train(recovery_time ~ ., 
            data = training_sec,
            method = "qda",
            metric = "ROC",
            trControl = ctrl1)
# Report the final model:
qda$finalModel

# Report the CV ROC:
qda$results$ROC
```

### Naive Bayes: 

```{r}
set.seed(3521)
nbGrid = expand.grid(usekernel = c(FALSE, TRUE),
                     fL = 1,
                     adjust = seq(.2, 3, by = .2))
nb = train(recovery_time ~ ., 
           data = training_sec, 
           method = "nb",
           tuneGrid = nbGrid,
           metric = "ROC",
           trControl = ctrl1)
# Plot for tuning parameter selection:
plot(nb, main = "Figure IV. NB Model CV ROC Plot", highlight = T)
# Report the final model:
nb$bestTune

# Report the CV ROC:
nb$results[26, ]$ROC
```



